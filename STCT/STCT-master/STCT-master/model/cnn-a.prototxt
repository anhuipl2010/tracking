name: "lnet"
force_backward: true
input: "data"
input_dim: 1
input_dim: 512
#input_dim: 232
#input_dim: 232
input_dim: 46 #48
input_dim: 46 #48
#input_dim: 480
#input_dim: 480
input: "w0"
input_dim: 1 
input_dim: 100
input_dim: 1
input_dim: 1

layer { 
  bottom: "data"
  top: "drop_data"
  name: "drop_data"
  type: "Dropout"
  dropout_param {
    dropout_ratio: 0.
  }
}

layer {
  bottom: "drop_data"
  top: "conv5_f1"
  name: "conv5_f1"
  type: "Convolution"
  param {
    lr_mult: 1
    decay_mult: 5
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    pad: 2
    mask: true
    kernel_size: 5
    weight_filler {
      type: "gaussian"
      std: 1e-7 #1e-7
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer{ 
  bottom: "conv5_f1"
  top: "conv5_f1"
  name: "relu5_f1"
  type: "ReLU"
}

layer {
  bottom: "conv5_f1"
  top: "conv5_f2"
  name: "conv5_f2"
  type: "Convolution"
  param {
    lr_mult: 1
    decay_mult: 5
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    group: 100
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 1e-7
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  bottom: "w0"
  top: "w"
  name: "w_reshape"
  type: "Reshape"
  reshape_param {
     shape {
       dim: 0
       dim: 100
     }
  }
}

layer {
  bottom: "conv5_f2"
  bottom: "w"
  top: "conv5_f2_drop"
  name: "conv5_f2_drop"
  type: "Scalar"
  scalar_param {
    axis: 0
  }
}
#layers{ 
#  bottom: "conv5_f2"
#  top: "conv5_f2"
#  name: "relu5_f2"
#  type: RELU
#
